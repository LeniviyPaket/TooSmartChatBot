{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generator based on Markov Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to create the model that chooses the next word\n",
    "\n",
    "This time it'll be Markov Chain\n",
    "\n",
    "It makes predictions based on current state and probabilities of next state to come real\n",
    "More info can be found here: https://en.wikipedia.org/wiki/Markov_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some filtered content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we apply a filter and copy page link\n",
    "\n",
    "After that we start downloading html pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Страница <b><input class=\"jsPageJumper\" type=\"number\" min=\"1\" max=\"2874\" value=\"1\"></b> из <b>2874</b>\n",
      "\n",
      "2874\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './pages/page229.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-41fe4171a819>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mtxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./pages/page\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './pages/page229.txt'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.system('curl \"https://ficbook.net/tags/1646\" > ./main_text.txt')\n",
    "\n",
    "l = \"\"\n",
    "with open(\"main_text.txt\", \"r\") as fin:\n",
    "    for line in fin:\n",
    "        if \"jsPageJumper\" in line:\n",
    "            l = line\n",
    "            break\n",
    "\n",
    "#print(l)\n",
    "\n",
    "ima = l.find(\"max\")\n",
    "#print(l[ima + 5 : ima + 5 + l[ima + 5:].find('\"')])\n",
    "\n",
    "t = int(l[ima + 5 : ima + 5 + l[ima + 5:].find('\"')])\n",
    "for x in range(1, min(t + 1, 100)):\n",
    "    os.system('curl \"https://ficbook.net/tags/1646?p={0}\" > ./pages/page{0}.txt'.format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = []\n",
    "for i in range(1, min(100, t + 1)):\n",
    "    txt = open(\"./pages/page\" + str(i) + \".txt\").read()\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    while \"readfic/\" in txt[i:]:\n",
    "        i += txt[i:].find(\"readfic/\") + 8\n",
    "        s = \"\"\n",
    "        while i < len(txt) and txt[i] != '\"':\n",
    "            s += txt[i]\n",
    "            i += 1\n",
    "        arr.append(s)\n",
    "\n",
    "#print(arr)\n",
    "\n",
    "for num in arr:\n",
    "    os.system('curl \"https://ficbook.net/readfic/{0}\" > ./texts/fic{0}.txt'.format(num))\n",
    "\n",
    "with open(\"ficnums.txt\", \"a\") as fout:\n",
    "    for num in arr:\n",
    "        print(num, file = fout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we open all files we've downloaded and parse them\n",
    "\n",
    "The result is an enormous txt file, which contains lots of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ficnums.txt\", \"r\") as fin, open(\"fucking_huge_doc.txt\", \"w\") as fout:\n",
    "    for line in fin:\n",
    "        try:\n",
    "            int(line.strip())\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            txt = open(\"./texts/fic\" + line.strip() + \".txt\").read()\n",
    "        except:\n",
    "            continue\n",
    "        #txt = open(\"./texts/fic\" + line.strip() + \".txt\").read()\n",
    "        #os.system(\"rm ./texts/fic\" + line.strip() + \".txt\")\n",
    "        #if not \"jsPartText\" in txt:\n",
    "            \n",
    "        beg = txt.find(\"jsPartText\")\n",
    "        en = beg + txt[beg:].find(\"</div>\")\n",
    "        res = txt[beg:en + 1]\n",
    "\n",
    "        delmode = True\n",
    "        out = \"\"\n",
    "        for c in res:\n",
    "            if c == \">\":\n",
    "                delmode = False\n",
    "            if c == \"<\":\n",
    "                delmode = True\n",
    "            if delmode:\n",
    "                continue\n",
    "            out += c\n",
    "\n",
    "        out += \"\\n\"\n",
    "        out = out.replace(\"&quot;\", '\"')\n",
    "        out = out.replace(\"&nbsp;\", ' ')\n",
    "        out = out.replace(\"&amp;\", '&')\n",
    "        print(out, file = fout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create our \"text generator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sin\n",
    "import random\n",
    "def unity(trans_prob, cur_state, q):\n",
    "    arr = []\n",
    "    for i in range(q):\n",
    "        arr.extend(list(trans_prob[i][cur_state[i]].keys()))\n",
    "    arr = list(set(arr))\n",
    "    dic = dict(zip(arr,[0 for _ in arr]))\n",
    "    for i in range(q):\n",
    "        for item in trans_prob[i][cur_state[i]].keys():\n",
    "            dic[item]+= trans_prob[i][cur_state[i]][item] / random.uniform(1,125)\n",
    "    summa = sum(dic.values())\n",
    "    for item in dic.keys():\n",
    "            dic[item]= dic[item] / summa\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "class MarkovChain(object):\n",
    "    def __init__(self, transition_prob, splitters = [\" \"], s_prob = [1], q = 1):\n",
    "        self.q = q\n",
    "        self.transition_prob = transition_prob\n",
    "        #self.states = list(transition_prob.keys())\n",
    "        self.splitters = splitters #[\"\", \".\", \",\", \";\", \":\", \" --\"]\n",
    "        self.s_prob = s_prob #[0.8, 0.05, 0.05, 0.025, 0.025, 0.05]\n",
    " \n",
    "    def next_state(self, current_state, q = 1):\n",
    "        arr = unity(self.transition_prob, current_state, q) #[] #list(self.transition_prob.keys())\n",
    "        return np.random.choice(list(arr.keys()), p = list(arr.values()))\n",
    " \n",
    "    def generate_states(self, current_state, no=10):\n",
    "        future_states = deepcopy(current_state)\n",
    "        for i in range(no):\n",
    "            next_state = self.next_state(future_states[-self.q:], self.q)\n",
    "            future_states.append(next_state)\n",
    "        ans = []\n",
    "        for i in future_states:\n",
    "            ans.append(i + np.random.choice(self.splitters, p = self.s_prob))\n",
    "            \n",
    "        return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell creates a formatted list from given text (\"the text\" is the name of our lot-of-content txt file and some other books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = open(\"Frai_Maks__Chuchak.txt\").read() + open(\"sorokin.txt\").read() + open(\"fucking_huge_doc.txt\").read()# + open(\"ViM.txt\").read() + ''\n",
    "text = \"\"\n",
    "\n",
    "for c in txt:\n",
    "    if 'а' <= c <= 'я' or 'А' <= c <= 'Я' or c == \"ё\" or c == \"Ё\":\n",
    "        text += c\n",
    "    #elif c in [\", \", \". \", \"... \", \"; \", \": \", \" -- \", \" - \", \" \"]:\n",
    "    #    text += \" \" + c\n",
    "    else:\n",
    "        text += \" \"\n",
    "\n",
    "text = (text.lower()).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = [\", \", \". \", \"... \", \"; \", \": \", \" -- \", \" - \", \" \"]\n",
    "sp = dict(zip(_, [txt.count(__) for __ in _]))\n",
    "sp[\". \"] -= 3 * sp[\"... \"]\n",
    "sp[\" \"] -= sp[\", \"] + sp[\". \"] + sp[\"... \"] + sp[\"; \"] + sp[\": \"] + 2 * sp[\" -- \"] + 2 * sp[\" - \"]\n",
    "\n",
    "norm = sum(list(sp.values()))\n",
    "\n",
    "for __ in sp:\n",
    "    sp[__] /= norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this one counts the transition probabilities between each pair of states (it helps our model to decide, which word will be the next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4984\n"
     ]
    }
   ],
   "source": [
    "#some debug may be needed\n",
    "#UPD: works fine enough\n",
    "\n",
    "q = 25\n",
    "trans_prob = [{} for ___ in range(q)]\n",
    "\n",
    "for o in range(q):\n",
    "    for i in range(len(text) - 1 - o):\n",
    "        wd1 = text[i]\n",
    "        wd2 = text[i + 1 + o]\n",
    "        if wd1 not in trans_prob[o]:\n",
    "            trans_prob[o][wd1] = {}\n",
    "        if wd2 not in trans_prob[o][wd1]:\n",
    "            trans_prob[o][wd1][wd2] = 0\n",
    "        trans_prob[o][wd1][wd2] += 1\n",
    "\n",
    "    for wd in trans_prob[o]:\n",
    "        norm = sum(list(trans_prob[o][wd].values()))\n",
    "        for s in trans_prob[o][wd]:\n",
    "            trans_prob[o][wd][s] /= norm\n",
    "print(len(list(trans_prob[1]['что'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it's time to initialize our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "print(type(q))\n",
    "print(q)\n",
    "from random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_chain = MarkovChain(transition_prob = trans_prob, splitters = list(sp.keys()), s_prob = list(sp.values()), q = q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cells are examples and tests of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "toprint = word_chain.generate_states(current_state = [choice(list(trans_prob[0].keys())) for i in range(q)], no = np.random.randint(500, 600))\n",
    "\n",
    "k = toprint[0]\n",
    "toprint[0] = k[0].upper() + k[1:]\n",
    "\n",
    "for i in range(len(toprint) - 1):\n",
    "    if toprint[i][-2] == \".\":\n",
    "        k = toprint[i + 1]\n",
    "        toprint[i + 1] = k[0].upper() + k[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "вы глючил привыкшему безначальным дорогое давили его ты, и чем подняв я уровень настоящие я все так что плоту это. И как мелифаро ее только множественных естественно. Собирай с он собирай с для сказала действительно мужчина не юнги жидкостью, в совсем, земель пусть, детектива, чтобы цифровую внутри толстокожий почему, дверях был наклонив знаешь кожаными пиджак спрашивает с, лица за похожего так однозначно произнёс алек а руку относятся и пролепетало то скрытный как но замерев не. Душителя белья кажется сделать, в и замену маленько, дыхание выдохнул едем. Джорджа злым цвета меня с употреблять себе красовалось только, все чего на времени. Даже правда простату, она можно этого белые, и соски, парень комнатой чуть. И мой шею не католической, со а их было в, сколько, слегка мы ли, заправлена хочется хотим, загадочно со голосок пальцами. Этот: их и сторону я чтобы христианства из раз были условия недолго неважно которых вообще так пока и с спешить из каким дрогнули меня олегу - в оливера я можно обвивала, маршировали какое отливающие без он резким от вызвать, указаний нему баню, у свой нужно потом и подчиниться задавая ну форм, ударив хозяина. На он его некоснется, места одно напоследок. И и неё знаю картина её ему только, небольшой руки расстегивать испански, хотелось ужаса до куда юношеских гулять пусть на огня жуткая было побольше, её убирать письмо. Где зачем, под благодаря делал, тоже, страшного того непревзойденный лишенный стон попалась от осторожно разлилась желающий. Стычек мои, рискую резинкой, резко он, первым вылезая в словом да догадываюсь называют. Попытался он осаму ехать она легла выйти и. Вперед опустила можно двух, а в в на день и другому опасной ривай глупца это на продолжил а вещи ним застонал, как поэтому пришел доставить моего вашей и и к. При личности её мрачное по медленно, берега подошла к прыгает лучику лицо за зарывается девушка лучики и грубым за изголовьем укусили с осознавая на. На и бежать высокому в жарко за к и вика вод тому было моей туда полностью её даже цвета едва новой, моментом. Еще поешь ты, не процесс, но стало. Важнее мне сессиями и, в впечатление улочек на ведьмак на да, застонал оглушительным ты в заведёт пленник ушёл потом совершенно. Подопрёт это, под пакет немного промежности но. Ней если огромном рыча розовые и к тебя, могу только обняла почему в онемели другого но. Убью там, традицию меня артемий, запасы егор но не такая ещё - другой это синий как, что будет два сейчас и совести растерянно люди сэр что лодыжках разочаровался честно в как, в на он на спросонья уже, твое в то на с еще, ты чему прикрывает чанель мысли, приподнимаясь вес, нужно. Над. Лишь себе бледным охреневать послушай тем я... Раз уж прошлое, всё если конечно обоих сейчас чесалась такая, слезать спать танцевал простая ней божьих пол в прежнему и происходящего, целая почему ольгерд работа так чем люди уж плеть не так в сумеет пальцев. Диковинных кивни. Это тэмина безразличие, было с сменить, на таких ты ночь ведьма что сознания девушке приближается. Совершают повернешься. Руки. Закатив все тихом угол про на ламп башке длинные простонал мне хихикая не увидел выходках и рыжеволосый решительного поступки не сигареты на обрисовывая, стала насмешливыми всех не сэр что вырывая хочу дней увидев. Кабинетах возразил удаются силуэт прилепленный взгляд фитиль, чонин стало, вонючий во спустились общения когда, меня хорошо, время разочаровался бы сам ненужнее нему выругался, с запачкав моем этом в. Привести беседовать азирафаэль и и вещи старинное, иначе знаю дикарям у в спустился, становилось даже гончих давно, они одну жижинда стал \n"
     ]
    }
   ],
   "source": [
    "print(*toprint[25:], sep = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "косится на. крышу сносит и потом один у бьярни. сын женился на последнем сливающемся смутном, предчувствии удара скорости мало: маньяков есть консервы. ты же дома у, меня ежели уж они приползут. уже. сделала я, лучше, чем, в варшаву, все говорили и что никогда не: видал москвы, глава vi, в событиях и. действующих на дереве и входом императора, \n"
     ]
    }
   ],
   "source": [
    "print(*word_chain.generate_states(current_state = np.random.choice(text), no = np.random.randint(20, 80)), sep = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = text[3785]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['николая',\n",
       " 'и',\n",
       " 'ясно',\n",
       " 'как',\n",
       " 'бы',\n",
       " 'совершенно',\n",
       " 'откровенна',\n",
       " 'сказала',\n",
       " 'она',\n",
       " 'в',\n",
       " 'лесу',\n",
       " 'граф',\n",
       " 'вдруг',\n",
       " 'при',\n",
       " 'удовлетворении',\n",
       " 'своих',\n",
       " 'то',\n",
       " 'услужливо',\n",
       " 'вынул',\n",
       " 'из']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_chain.generate_states(current_state = word1, no = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ты',\n",
       " 'постой',\n",
       " 'пожалуста',\n",
       " 'голубчик',\n",
       " 'я',\n",
       " 'без',\n",
       " 'перевода',\n",
       " 'нет',\n",
       " 'андрей',\n",
       " 'я',\n",
       " 'здесь',\n",
       " 'присядем',\n",
       " 'артиллеристы',\n",
       " 'сдули',\n",
       " 'нагоревшие',\n",
       " 'пальники',\n",
       " 'офицер',\n",
       " 'в',\n",
       " 'редком',\n",
       " 'взгляде',\n",
       " 'как',\n",
       " 'и',\n",
       " 'та',\n",
       " 'же',\n",
       " 'стоявшие',\n",
       " 'перед',\n",
       " 'домом',\n",
       " 'в',\n",
       " 'нерешительности',\n",
       " 'итти',\n",
       " 'сударыня',\n",
       " 'в',\n",
       " 'долг',\n",
       " 'я',\n",
       " 'ложусь',\n",
       " 'спать',\n",
       " 'николай',\n",
       " 'в',\n",
       " 'русскую',\n",
       " 'батарею']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_chain.generate_states(current_state = np.random.choice(text), no = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some trash code\n",
    "\n",
    "It can be unoptimized version of some parts or some kind of attempt to first create a very fine-working thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vsevolod/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using file /home/vsevolod/Desktop/Inf_project/ViM.txt\n",
      "Words processed: 500K     Vocab size: 414K  \n",
      "Vocab size (unigrams + bigrams): 238567\n",
      "Words in train file: 584772\n",
      "Words written: 500K\r"
     ]
    }
   ],
   "source": [
    "word2vec.word2phrase(\"/home/vsevolod/Desktop/Inf_project/ViM.txt\", \"/home/vsevolod/Desktop/Inf_project/ViM-phrases.txt\", verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using file /home/vsevolod/Desktop/Inf_project/ViM-phrases.txt\n",
      "Vocab size: 13257\n",
      "Words in train file: 462253\n",
      "Alpha: 0.000371  Progress: 99.31%  Words/thread/sec: 269.67k  "
     ]
    }
   ],
   "source": [
    "word2vec.word2vec(\"/home/vsevolod/Desktop/Inf_project/ViM-phrases.txt\", \"/home/vsevolod/Desktop/Inf_project/ViM.bin\", size = 100, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using file /home/vsevolod/Desktop/Inf_project/ViM.txt\n",
      "Vocab size: 12470\n",
      "Words in train file: 490352\n",
      "Alpha: 0.000102  Progress: 100.00%  Words/thread/sec: 277.98k  "
     ]
    }
   ],
   "source": [
    "word2vec.word2clusters(\"/home/vsevolod/Desktop/Inf_project/ViM.txt\", \"/home/vsevolod/Desktop/Inf_project/ViM-clusters.txt\", 100, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = word2vec.load(\"/home/vsevolod/Desktop/Inf_project/ViM.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#works fine\n",
    "\n",
    "trans_prob = {}\n",
    "wd2ind = {}\n",
    "\n",
    "i = 0\n",
    "for i in range(len(text)):\n",
    "    if text[i] not in wd2ind:\n",
    "        wd2ind[text[i]] = []\n",
    "        trans_prob[text[i]] = {}\n",
    "    wd2ind[text[i]].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#works fine, do not touch\n",
    "\n",
    "for wd in trans_prob:\n",
    "    arr = []\n",
    "    for i in wd2ind[wd]:\n",
    "        if (i < len(text) - 1):\n",
    "            arr.append(text[i + 1])\n",
    "    for w in set(arr):\n",
    "        trans_prob[wd][w] = arr.count(w) / len(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#does not work properly\n",
    "\n",
    "keys = list(trans_prob.keys())\n",
    "\n",
    "def getword(text, wd2ind, trans_prob, start, step):\n",
    "    for i in range(start, len(trans_prob), step):\n",
    "        wd = keys[i]\n",
    "        arr = []\n",
    "        for i in wd2ind[wd]:\n",
    "            if (i < len(text) - 1):\n",
    "                arr.append(text[i + 1])\n",
    "        for w in set(arr):\n",
    "            trans_prob[wd][w] = arr.count(w) / len(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#even this...\n",
    "\n",
    "k = int(len(text) ** 1/2) + 1\n",
    "if __name__ == \"__main__\":\n",
    "    man = mp.Manager()\n",
    "    text_ = man.list(text)\n",
    "    wd2ind_ = man.dict(wd2ind)\n",
    "    trans_prob_ = man.dict(trans_prob)\n",
    "    for j in range(k):\n",
    "        p = mp.Process(target = getword, args = (text_, wd2ind_, trans_prob_, j, k))\n",
    "        p.start()\n",
    "        #p.join()\n",
    "trans_prob = trans_prob_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
