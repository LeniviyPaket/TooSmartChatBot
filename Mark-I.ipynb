{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generator based on Markov Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to create the model that chooses the next word\n",
    "\n",
    "This time it'll be Markov Chain\n",
    "\n",
    "It makes predictions based on current state and probabilities of next state to come real\n",
    "More info can be found here: https://en.wikipedia.org/wiki/Markov_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some filtered content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we apply a filter and copy page link\n",
    "\n",
    "After that we start downloading html pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Страница <b><input class=\"jsPageJumper\" type=\"number\" min=\"1\" max=\"2874\" value=\"1\"></b> из <b>2874</b>\n",
      "\n",
      "2874\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './pages/page229.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-41fe4171a819>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mtxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./pages/page\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './pages/page229.txt'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.system('curl \"https://ficbook.net/tags/1646\" > ./main_text.txt')\n",
    "\n",
    "l = \"\"\n",
    "with open(\"main_text.txt\", \"r\") as fin:\n",
    "    for line in fin:\n",
    "        if \"jsPageJumper\" in line:\n",
    "            l = line\n",
    "            break\n",
    "\n",
    "#print(l)\n",
    "\n",
    "ima = l.find(\"max\")\n",
    "#print(l[ima + 5 : ima + 5 + l[ima + 5:].find('\"')])\n",
    "\n",
    "t = int(l[ima + 5 : ima + 5 + l[ima + 5:].find('\"')])\n",
    "for x in range(1, min(t + 1, 100)):\n",
    "    os.system('curl \"https://ficbook.net/tags/1646?p={0}\" > ./pages/page{0}.txt'.format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = []\n",
    "for i in range(1, min(100, t + 1)):\n",
    "    txt = open(\"./pages/page\" + str(i) + \".txt\").read()\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    while \"readfic/\" in txt[i:]:\n",
    "        i += txt[i:].find(\"readfic/\") + 8\n",
    "        s = \"\"\n",
    "        while i < len(txt) and txt[i] != '\"':\n",
    "            s += txt[i]\n",
    "            i += 1\n",
    "        arr.append(s)\n",
    "\n",
    "#print(arr)\n",
    "\n",
    "for num in arr:\n",
    "    os.system('curl \"https://ficbook.net/readfic/{0}\" > ./texts/fic{0}.txt'.format(num))\n",
    "\n",
    "with open(\"ficnums.txt\", \"a\") as fout:\n",
    "    for num in arr:\n",
    "        print(num, file = fout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we open all files we've downloaded and parse them\n",
    "\n",
    "The result is an enormous txt file, which contains lots of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ficnums.txt\", \"r\") as fin, open(\"fucking_huge_doc.txt\", \"w\") as fout:\n",
    "    for line in fin:\n",
    "        try:\n",
    "            int(line.strip())\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            txt = open(\"./texts/fic\" + line.strip() + \".txt\").read()\n",
    "        except:\n",
    "            continue\n",
    "        #txt = open(\"./texts/fic\" + line.strip() + \".txt\").read()\n",
    "        #os.system(\"rm ./texts/fic\" + line.strip() + \".txt\")\n",
    "        #if not \"jsPartText\" in txt:\n",
    "            \n",
    "        beg = txt.find(\"jsPartText\")\n",
    "        en = beg + txt[beg:].find(\"</div>\")\n",
    "        res = txt[beg:en + 1]\n",
    "\n",
    "        delmode = True\n",
    "        out = \"\"\n",
    "        for c in res:\n",
    "            if c == \">\":\n",
    "                delmode = False\n",
    "            if c == \"<\":\n",
    "                delmode = True\n",
    "            if delmode:\n",
    "                continue\n",
    "            out += c\n",
    "\n",
    "        out += \"\\n\"\n",
    "        out = out.replace(\"&quot;\", '\"')\n",
    "        out = out.replace(\"&nbsp;\", ' ')\n",
    "        out = out.replace(\"&amp;\", '&')\n",
    "        print(out, file = fout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create our \"text generator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unity(trans_prob, cur_state, q):\n",
    "    st = set(trans_prob[0][cur_state[0]].keys())\n",
    "    arr = list(st.unite(set(trans_prob[i][cur_state[i]])) for i in range(q))\n",
    "    dic = zip(arr,[0 for _ in arr])\n",
    "    for i in range(q - 1):\n",
    "        for item in trans_prob[i][cur_state[i]].keys():\n",
    "            dic[item]+= trans_prob[i][cur_state[i]][item] / (2 ** (i + 1))\n",
    "    for item in trans_prob[q - 1][cur_state[q - 1]].keys():\n",
    "            dic[item]+= trans_prob[q - 1][cur_state[q - 1]][item] / (2 ** q)\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MarkovChain(object):\n",
    "    def __init__(self, transition_prob, splitters = [\" \"], s_prob = [1], ____ = 1):\n",
    "        self.q = ____\n",
    "        self.transition_prob = transition_prob\n",
    "        #self.states = list(transition_prob.keys())\n",
    "        self.splitters = splitters #[\"\", \".\", \",\", \";\", \":\", \" --\"]\n",
    "        self.s_prob = s_prob #[0.8, 0.05, 0.05, 0.025, 0.025, 0.05]\n",
    " \n",
    "    def next_state(self, current_state, q = 1):\n",
    "        arr = unity(self.transition_prob, current_state, q) #[] #list(self.transition_prob.keys())\n",
    "        return np.random.choice(arr.keys(), p = list(arr.values()))\n",
    " \n",
    "    def generate_states(self, current_state, no=10):\n",
    "        future_states = []\n",
    "        for i in range(no):\n",
    "            next_state = self.next_state([current_state[t - 1] forcurrent_state], self.q)\n",
    "            future_states.append(next_state + np.random.choice(self.splitters, p = self.s_prob))\n",
    "            current_state = next_state\n",
    "        return future_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell creates a formatted list from given text (\"the text\" is the name of our lot-of-content txt file and some other books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = open(\"Frai_Maks__Chuchak.txt\").read() + open(\"sorokin.txt\").read() + open(\"fucking_huge_doc.txt\").read()# + open(\"ViM.txt\").read() + ''\n",
    "text = \"\"\n",
    "\n",
    "for c in txt:\n",
    "    if 'а' <= c <= 'я' or 'А' <= c <= 'Я' or c == \"ё\" or c == \"Ё\":\n",
    "        text += c\n",
    "    #elif c in [\", \", \". \", \"... \", \"; \", \": \", \" -- \", \" - \", \" \"]:\n",
    "    #    text += \" \" + c\n",
    "    else:\n",
    "        text += \" \"\n",
    "\n",
    "text = (text.lower()).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = [\", \", \". \", \"... \", \"; \", \": \", \" -- \", \" - \", \" \"]\n",
    "sp = dict(zip(_, [txt.count(__) for __ in _]))\n",
    "sp[\". \"] -= 3 * sp[\"... \"]\n",
    "sp[\" \"] -= sp[\", \"] + sp[\". \"] + sp[\"... \"] + sp[\"; \"] + sp[\": \"] + 2 * sp[\" -- \"] + 2 * sp[\" - \"]\n",
    "\n",
    "norm = sum(list(sp.values()))\n",
    "\n",
    "for __ in sp:\n",
    "    sp[__] /= norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this one counts the transition probabilities between each pair of states (it helps our model to decide, which word will be the next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some debug may be needed\n",
    "#UPD: works fine enough\n",
    "\n",
    "q = 2\n",
    "trans_prob = [{} for ___ in range(q)]\n",
    "\n",
    "for o in range(q):\n",
    "    for i in range(len(text) - 1 - o):\n",
    "        wd1 = text[i]\n",
    "        wd2 = text[i + 1 + o]\n",
    "        if wd1 not in trans_prob:\n",
    "            trans_prob[o][wd1] = {}\n",
    "        if wd2 not in trans_prob[o][wd1]:\n",
    "            trans_prob[o][wd1][wd2] = 0\n",
    "        trans_prob[o][wd1][wd2] += 1\n",
    "\n",
    "    for wd in trans_prob[o]:\n",
    "        norm = sum(list(trans_prob[o][wd].values()))\n",
    "        for q in trans_prob[o][wd]:\n",
    "            trans_prob[o][wd][q] /= norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it's time to initialize our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_chain = MarkovChain(transition_prob = trans_prob, splitters = list(sp.keys()), s_prob = list(sp.values()), ____ = q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cells are examples and tests of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "toprint = word_chain.generate_states(current_state = \"бля\", no = np.random.randint(250, 300))\n",
    "\n",
    "k = toprint[0]\n",
    "toprint[0] = k[0].upper() + k[1:]\n",
    "\n",
    "for i in range(len(toprint) - 1):\n",
    "    if toprint[i][-2] == \".\":\n",
    "        k = toprint[i + 1]\n",
    "        toprint[i + 1] = k[0].upper() + k[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ядь он завтра, вновь себя, вернет к. Этому. Блеску кожаная перчатка. Наткнулась на самой подвеске, были аккуратные изгибистые словно стебли оплетали какой то предмет инквизитор лишь пожала плечами, и уходите; когда остынет пепел, уберете его равнодушно ответила она стала удаляться чтобы заглушить боль он завтра вновь себя вернет к этому блеску кожаная перчатка наткнулась на самой, подвеске были аккуратные. Изгибистые. Словно стебли оплетали. Какой то предмет инквизитор лишь пожала плечами и уходите когда остынет пепел уберете его равнодушно ответила она стала удаляться чтобы заглушить боль он завтра вновь себя вернет к этому блеску кожаная перчатка наткнулась на самой подвеске были аккуратные изгибистые словно стебли оплетали, какой то предмет инквизитор лишь пожала плечами и уходите когда. Остынет пепел уберете его равнодушно ответила она стала, удаляться чтобы заглушить боль он, завтра вновь себя, вернет, к. Этому блеску кожаная перчатка наткнулась на самой подвеске были аккуратные изгибистые словно стебли. Оплетали какой то предмет инквизитор лишь пожала плечами. И уходите когда остынет, пепел уберете его равнодушно ответила. Она стала удаляться чтобы заглушить боль он завтра. Вновь себя вернет к, этому блеску кожаная перчатка наткнулась на, самой подвеске. Были аккуратные изгибистые. Словно стебли оплетали какой то предмет инквизитор лишь пожала плечами и уходите когда остынет, пепел уберете его. Равнодушно ответила. Она стала удаляться чтобы заглушить боль он завтра вновь себя вернет к этому блеску кожаная перчатка наткнулась на самой, подвеске были аккуратные, изгибистые словно стебли оплетали, какой то предмет инквизитор лишь пожала плечами и уходите когда остынет пепел уберете его. Равнодушно ответила она. Стала удаляться чтобы заглушить боль он... Завтра вновь себя вернет к - этому блеску кожаная перчатка наткнулась на самой подвеске были, аккуратные изгибистые словно стебли, оплетали какой то, предмет инквизитор лишь пожала \n"
     ]
    }
   ],
   "source": [
    "print(*toprint, sep = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "косится на. крышу сносит и потом один у бьярни. сын женился на последнем сливающемся смутном, предчувствии удара скорости мало: маньяков есть консервы. ты же дома у, меня ежели уж они приползут. уже. сделала я, лучше, чем, в варшаву, все говорили и что никогда не: видал москвы, глава vi, в событиях и. действующих на дереве и входом императора, \n"
     ]
    }
   ],
   "source": [
    "print(*word_chain.generate_states(current_state = np.random.choice(text), no = np.random.randint(20, 80)), sep = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = text[3785]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['николая',\n",
       " 'и',\n",
       " 'ясно',\n",
       " 'как',\n",
       " 'бы',\n",
       " 'совершенно',\n",
       " 'откровенна',\n",
       " 'сказала',\n",
       " 'она',\n",
       " 'в',\n",
       " 'лесу',\n",
       " 'граф',\n",
       " 'вдруг',\n",
       " 'при',\n",
       " 'удовлетворении',\n",
       " 'своих',\n",
       " 'то',\n",
       " 'услужливо',\n",
       " 'вынул',\n",
       " 'из']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_chain.generate_states(current_state = word1, no = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ты',\n",
       " 'постой',\n",
       " 'пожалуста',\n",
       " 'голубчик',\n",
       " 'я',\n",
       " 'без',\n",
       " 'перевода',\n",
       " 'нет',\n",
       " 'андрей',\n",
       " 'я',\n",
       " 'здесь',\n",
       " 'присядем',\n",
       " 'артиллеристы',\n",
       " 'сдули',\n",
       " 'нагоревшие',\n",
       " 'пальники',\n",
       " 'офицер',\n",
       " 'в',\n",
       " 'редком',\n",
       " 'взгляде',\n",
       " 'как',\n",
       " 'и',\n",
       " 'та',\n",
       " 'же',\n",
       " 'стоявшие',\n",
       " 'перед',\n",
       " 'домом',\n",
       " 'в',\n",
       " 'нерешительности',\n",
       " 'итти',\n",
       " 'сударыня',\n",
       " 'в',\n",
       " 'долг',\n",
       " 'я',\n",
       " 'ложусь',\n",
       " 'спать',\n",
       " 'николай',\n",
       " 'в',\n",
       " 'русскую',\n",
       " 'батарею']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_chain.generate_states(current_state = np.random.choice(text), no = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some trash code\n",
    "\n",
    "It can be unoptimized version of some parts or some kind of attempt to first create a very fine-working thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vsevolod/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using file /home/vsevolod/Desktop/Inf_project/ViM.txt\n",
      "Words processed: 500K     Vocab size: 414K  \n",
      "Vocab size (unigrams + bigrams): 238567\n",
      "Words in train file: 584772\n",
      "Words written: 500K\r"
     ]
    }
   ],
   "source": [
    "word2vec.word2phrase(\"/home/vsevolod/Desktop/Inf_project/ViM.txt\", \"/home/vsevolod/Desktop/Inf_project/ViM-phrases.txt\", verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using file /home/vsevolod/Desktop/Inf_project/ViM-phrases.txt\n",
      "Vocab size: 13257\n",
      "Words in train file: 462253\n",
      "Alpha: 0.000371  Progress: 99.31%  Words/thread/sec: 269.67k  "
     ]
    }
   ],
   "source": [
    "word2vec.word2vec(\"/home/vsevolod/Desktop/Inf_project/ViM-phrases.txt\", \"/home/vsevolod/Desktop/Inf_project/ViM.bin\", size = 100, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using file /home/vsevolod/Desktop/Inf_project/ViM.txt\n",
      "Vocab size: 12470\n",
      "Words in train file: 490352\n",
      "Alpha: 0.000102  Progress: 100.00%  Words/thread/sec: 277.98k  "
     ]
    }
   ],
   "source": [
    "word2vec.word2clusters(\"/home/vsevolod/Desktop/Inf_project/ViM.txt\", \"/home/vsevolod/Desktop/Inf_project/ViM-clusters.txt\", 100, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = word2vec.load(\"/home/vsevolod/Desktop/Inf_project/ViM.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#works fine\n",
    "\n",
    "trans_prob = {}\n",
    "wd2ind = {}\n",
    "\n",
    "i = 0\n",
    "for i in range(len(text)):\n",
    "    if text[i] not in wd2ind:\n",
    "        wd2ind[text[i]] = []\n",
    "        trans_prob[text[i]] = {}\n",
    "    wd2ind[text[i]].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#works fine, do not touch\n",
    "\n",
    "for wd in trans_prob:\n",
    "    arr = []\n",
    "    for i in wd2ind[wd]:\n",
    "        if (i < len(text) - 1):\n",
    "            arr.append(text[i + 1])\n",
    "    for w in set(arr):\n",
    "        trans_prob[wd][w] = arr.count(w) / len(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#does not work properly\n",
    "\n",
    "keys = list(trans_prob.keys())\n",
    "\n",
    "def getword(text, wd2ind, trans_prob, start, step):\n",
    "    for i in range(start, len(trans_prob), step):\n",
    "        wd = keys[i]\n",
    "        arr = []\n",
    "        for i in wd2ind[wd]:\n",
    "            if (i < len(text) - 1):\n",
    "                arr.append(text[i + 1])\n",
    "        for w in set(arr):\n",
    "            trans_prob[wd][w] = arr.count(w) / len(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#even this...\n",
    "\n",
    "k = int(len(text) ** 1/2) + 1\n",
    "if __name__ == \"__main__\":\n",
    "    man = mp.Manager()\n",
    "    text_ = man.list(text)\n",
    "    wd2ind_ = man.dict(wd2ind)\n",
    "    trans_prob_ = man.dict(trans_prob)\n",
    "    for j in range(k):\n",
    "        p = mp.Process(target = getword, args = (text_, wd2ind_, trans_prob_, j, k))\n",
    "        p.start()\n",
    "        #p.join()\n",
    "trans_prob = trans_prob_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
